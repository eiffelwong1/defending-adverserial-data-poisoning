{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "y_train = tf.keras.utils.to_categorical(y_train)\n",
    "y_test = tf.keras.utils.to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 32, 32, 3)\n",
      "(50000, 10)\n",
      "(10000, 32, 32, 3)\n",
      "(10000, 10)\n",
      "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)\n",
    "\n",
    "print(y_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding_layer_iyswim(inputs, shape, name=None):\n",
    "    h_start = shape[0]\n",
    "    w_start = shape[1]\n",
    "    output_short = shape[2]\n",
    "    input_shape = tf.shape(inputs)\n",
    "    input_short = tf.reduce_min(input_shape[1:3])\n",
    "    input_long = tf.reduce_max(input_shape[1:3])\n",
    "    output_long = tf.to_int32(tf.ceil(\n",
    "        1. * tf.to_float(output_short) * tf.to_float(input_long) / tf.to_float(input_short)))\n",
    "    output_height = tf.to_int32(input_shape[1] >= input_shape[2]) * output_long +\\\n",
    "        tf.to_int32(input_shape[1] < input_shape[2]) * output_short\n",
    "    output_width = tf.to_int32(input_shape[1] >= input_shape[2]) * output_short +\\\n",
    "        tf.to_int32(input_shape[1] < input_shape[2]) * output_long\n",
    "    return tf.pad(inputs, tf.to_int32(tf.stack([[0, 0], [h_start, output_height - h_start - input_shape[1]], [w_start, output_width - w_start - input_shape[2]], [0, 0]])), name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "mypath = \"./adversarial_poisons/img/data\"\n",
    "onlyfiles = [f for f in listdir(mypath) if isfile(join(mypath, f))]\n",
    "\n",
    "onlyfiles = sorted(onlyfiles, key = lambda x: int(x[:-4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000\n"
     ]
    }
   ],
   "source": [
    "print(len(onlyfiles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-21 00:29:24.819235: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-11-21 00:29:25.480370: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9659 MB memory:  -> device: 0, name: GeForce RTX 2080 Ti, pci bus id: 0000:61:00.0, compute capability: 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 32, 32, 3)]       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 32, 32, 64)        1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 32, 32, 64)        36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 16, 16, 128)       73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 16, 16, 128)       147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 8, 8, 256)         295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 8, 8, 256)         590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 8, 8, 256)         590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 4, 4, 512)         1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 4, 4, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 4, 4, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 2, 2, 512)         0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 2, 2, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 2, 2, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 2, 2, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 1, 1, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 4096)              2101248   \n",
      "_________________________________________________________________\n",
      "fc2 (Dense)                  (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "predictions (Dense)          (None, 10)                40970     \n",
      "=================================================================\n",
      "Total params: 33,638,218\n",
      "Trainable params: 33,638,218\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.applications.vgg16.VGG16(weights=None,\n",
    "    input_shape=(32,32,3), pooling=None, classes=10,\n",
    "    classifier_activation='softmax'\n",
    ")\n",
    "\n",
    "#sgd = optimizers.SGD(lr=learning_rate, decay=lr_decay, momentum=0.9, nesterov=True)\n",
    "#        model.compile(loss='categorical_crossentropy', optimizer=sgd,metrics=['accuracy'])\n",
    "\n",
    "model.compile(optimizer='SGD', \\\n",
    "              loss='categorical_crossentropy',\\\n",
    "              metrics=[\"acc\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-21 00:29:26.709239: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-21 00:29:27.900794: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8201\n",
      "2021-11-21 00:29:28.520835: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2021-11-21 00:29:28.521618: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2021-11-21 00:29:28.521633: W tensorflow/stream_executor/gpu/asm_compiler.cc:77] Couldn't get ptxas version string: Internal: Couldn't invoke ptxas --version\n",
      "2021-11-21 00:29:28.523651: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2021-11-21 00:29:28.523726: W tensorflow/stream_executor/gpu/redzone_allocator.cc:314] Internal: Failed to launch ptxas\n",
      "Relying on driver to perform ptx compilation. \n",
      "Modify $PATH to customize ptxas location.\n",
      "This message will be only logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1407/1407 [==============================] - 34s 21ms/step - loss: 1.9362 - acc: 0.2875 - val_loss: 1.6995 - val_acc: 0.3558\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.69952, saving model to best.hdf5\n",
      "Epoch 2/1000\n",
      "1407/1407 [==============================] - 28s 20ms/step - loss: 1.4832 - acc: 0.4607 - val_loss: 1.3761 - val_acc: 0.4918\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.69952 to 1.37608, saving model to best.hdf5\n",
      "Epoch 3/1000\n",
      "1407/1407 [==============================] - 28s 20ms/step - loss: 1.2166 - acc: 0.5645 - val_loss: 1.7145 - val_acc: 0.3636\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.37608\n",
      "Epoch 4/1000\n",
      "1407/1407 [==============================] - 28s 20ms/step - loss: 1.0126 - acc: 0.6409 - val_loss: 1.5042 - val_acc: 0.4460\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.37608\n",
      "Epoch 5/1000\n",
      "1407/1407 [==============================] - 28s 20ms/step - loss: 0.8525 - acc: 0.7033 - val_loss: 0.9680 - val_acc: 0.6636\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.37608 to 0.96803, saving model to best.hdf5\n",
      "Epoch 6/1000\n",
      "1407/1407 [==============================] - 28s 20ms/step - loss: 0.7205 - acc: 0.7481 - val_loss: 1.0063 - val_acc: 0.6784\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.96803\n",
      "Epoch 7/1000\n",
      "1407/1407 [==============================] - 28s 20ms/step - loss: 0.6036 - acc: 0.7896 - val_loss: 0.8564 - val_acc: 0.7228\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.96803 to 0.85639, saving model to best.hdf5\n",
      "Epoch 8/1000\n",
      "1407/1407 [==============================] - 28s 20ms/step - loss: 0.5044 - acc: 0.8238 - val_loss: 0.9318 - val_acc: 0.7138\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.85639\n",
      "Epoch 9/1000\n",
      "1407/1407 [==============================] - 28s 20ms/step - loss: 0.4128 - acc: 0.8550 - val_loss: 0.7719 - val_acc: 0.7516\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.85639 to 0.77190, saving model to best.hdf5\n",
      "Epoch 10/1000\n",
      "1407/1407 [==============================] - 28s 20ms/step - loss: 0.3438 - acc: 0.8812 - val_loss: 0.7748 - val_acc: 0.7676\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.77190\n",
      "Epoch 11/1000\n",
      "1407/1407 [==============================] - 28s 20ms/step - loss: 0.2794 - acc: 0.9032 - val_loss: 0.9439 - val_acc: 0.7354\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.77190\n",
      "Epoch 12/1000\n",
      "1407/1407 [==============================] - 28s 20ms/step - loss: 0.2302 - acc: 0.9201 - val_loss: 1.0192 - val_acc: 0.7312\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.77190\n",
      "Epoch 13/1000\n",
      "1407/1407 [==============================] - 28s 20ms/step - loss: 0.1906 - acc: 0.9348 - val_loss: 0.9006 - val_acc: 0.7670\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.77190\n",
      "Epoch 14/1000\n",
      "1407/1407 [==============================] - 28s 20ms/step - loss: 0.1557 - acc: 0.9465 - val_loss: 0.9308 - val_acc: 0.7790\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.77190\n",
      "Epoch 00014: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-21 00:36:05.003561: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_model/vgg16_32_32_ep100_val10per/assets\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "CHECKPOINT_PATH = 'best.hdf5'\n",
    "\n",
    "es = EarlyStopping(monitor = 'val_loss', mode = 'min', verbose = 1, patience = 5)\n",
    "cp = ModelCheckpoint(CHECKPOINT_PATH, monitor = 'val_loss', mode = 'min', verbose = 1, save_best_only = True)\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs = 1000, validation_split = 0.1, callbacks=[es,cp])\n",
    "model.save('saved_model/vgg16_32_32_ep100_val10per')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
